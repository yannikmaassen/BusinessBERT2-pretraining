# Configuration for BERT Pretraining

# Model configuration
model:
  name: "bert-base-uncased"
  max_length: 512
  mlm_probability: 0.15

# Training configuration
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 10000
  logging_steps: 100
  save_steps: 5000
  eval_steps: 5000
  save_total_limit: 3
  fp16: true
  dataloader_num_workers: 4
  seed: 42

# Dataset configuration
dataset:
  train_split: 0.9
  validation_split: 0.1
  nsp_probability: 0.5
  short_seq_probability: 0.1
  max_sentences_per_doc: 100

# Logging
logging:
  report_to: ["tensorboard"]
  logging_dir: "./logs"

