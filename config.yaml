# Configuration for BERT Pretraining

# Model configuration
model:
  name: "bert-base-uncased"
  max_length: 512
  mlm_probability: 0.15

# Training configuration
training:
  output_dir: "./outputs"
  num_train_epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 5.0e-5
  weight_decay: 0.01
  warmup_steps: 10000
  logging_steps: 100
  save_steps: 5000
  eval_steps: 5000
  save_total_limit: 3
  fp16: true
  dataloader_num_workers: 4
  seed: 42

# Dataset configuration
dataset:
  train_split: 0.95
  validation_split: 0.05
  nsp_probability: 0.5
  short_seq_probability: 0.1
  max_sentences_per_doc: 100

# Logging
logging:
  report_to: ["wandb", "tensorboard"]
  logging_dir: "./logs"

# Weights & Biases configuration
wandb:
  project: "businessbert-pretraining"
  entity: null  # Set to your W&B username/team or leave null for default
  name: null  # Run name - will auto-generate if null
  tags: ["bert", "pretraining", "mlm", "nsp"]
  notes: "BERT pretraining with MLM and NSP objectives on business documents"
  log_model: "checkpoint"  # Options: false, "checkpoint", "end"
